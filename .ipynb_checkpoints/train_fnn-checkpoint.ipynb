{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "edeefd05-6122-4aa1-a828-64199de4c63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import json\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import random\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6b182eb-b060-4999-abfb-c91da50415a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Vishnu\n",
      "[nltk_data]     Priya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Vishnu\n",
      "[nltk_data]     Priya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "stemmer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46363eee-8a8f-4795-92b7-69bc2b4ce359",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"intents_big.json\", \"r\") as f:\n",
    "    intents = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3456ceb-a644-48c6-99f1-95308b2ad4c2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pattern'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 8\u001b[0m\n\u001b[0;32m      4\u001b[0m ignore_words \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m?\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m intent \u001b[38;5;129;01min\u001b[39;00m intents[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mintents\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n\u001b[1;32m----> 8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m pattern \u001b[38;5;129;01min\u001b[39;00m \u001b[43mintent\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpattern\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m:\n\u001b[0;32m      9\u001b[0m         \u001b[38;5;66;03m# Tokenize each word in the sentence\u001b[39;00m\n\u001b[0;32m     10\u001b[0m         w \u001b[38;5;241m=\u001b[39m nltk\u001b[38;5;241m.\u001b[39mword_tokenize(pattern)\n\u001b[0;32m     11\u001b[0m         \u001b[38;5;66;03m# Add to our words list\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'pattern'"
     ]
    }
   ],
   "source": [
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore_words = ['?']\n",
    "\n",
    "for intent in intents['intents']:\n",
    "    if 'patterns' in intent:  # Check if the key exists\n",
    "        for pattern in intent['patterns']:\n",
    "            # Tokenize each word in the sentence\n",
    "            w = nltk.word_tokenize(pattern)\n",
    "            # Add to our words list\n",
    "            words.extend(w)\n",
    "            # Add to documents in our corpus\n",
    "            documents.append((w, intent['tag']))\n",
    "            # Add to our classes list\n",
    "            if intent['tag'] not in classes:\n",
    "                classes.append(intent['tag'])\n",
    "    else:\n",
    "        print(f\"Warning: 'patterns' key missing in intent: {intent}\")\n",
    "        \n",
    "words = [stemmer.lemmatize(w.lower()) for w in words if w not in ignore_words]\n",
    "words = sorted(list(set(words)))\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print(len(documents), \"documents\")\n",
    "print(len(classes), \"classes\", classes)\n",
    "print(len(words), \"unique lemmatized words\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915893a3-88b9-4d2b-bd77-4f284f1b3b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = []\n",
    "output = []\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# Training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # Initialize our bag of words\n",
    "    bag = []\n",
    "    # List of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # Lemmatize each word\n",
    "    pattern_words = [stemmer.lemmatize(word.lower()) for word in pattern_words]\n",
    "    # Create our bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # Output is a '0' for each tag and '1' for the current tag\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "random.shuffle(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b496a3-7278-4ee5-b469-80b2adf85d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_replacement(tokens, limit):\n",
    "    augmented_sentences = []\n",
    "    for i in range(len(tokens)):\n",
    "        synonyms = []\n",
    "        for syn in wordnet.synsets(tokens[i]):\n",
    "            for lemma in syn.lemmas():\n",
    "                synonyms.append(lemma.name())\n",
    "        if len(synonyms) > 0:\n",
    "            num_augmentations = min(limit, len(synonyms))\n",
    "            sampled_synonyms = random.sample(synonyms, num_augmentations)\n",
    "            for synonym in sampled_synonyms:\n",
    "                augmented_tokens = tokens[:i] + [synonym] + tokens[i + 1:]\n",
    "                augmented_sentences.append(' '.join(augmented_tokens))\n",
    "    return augmented_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2a0f33-95fc-411b-b555-177dce5bbac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augment the training data using synonym replacement\n",
    "augmented_data = []\n",
    "limit_per_tag = 100\n",
    "\n",
    "for i, doc in enumerate(training):\n",
    "    bag, output_row = doc\n",
    "    tokens = [words[j] for j in range(len(words)) if bag[j] == 1]\n",
    "    augmented_sentences = synonym_replacement(tokens, limit_per_tag)\n",
    "    for augmented_sentence in augmented_sentences:\n",
    "        augmented_bag = [1 if augmented_sentence.find(word) >= 0 else 0 for word in words]\n",
    "        augmented_data.append([augmented_bag, output_row])\n",
    "\n",
    "\n",
    "combined_data = np.concatenate((training, augmented_data), axis=0)\n",
    "random.shuffle(combined_data)\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027e361b-1444-4941-af81-30d4c275e195",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_data_by_tags(data):\n",
    "    data_by_tags = {}\n",
    "    for d in data:\n",
    "        tag = tuple(d[1])\n",
    "        if tag not in data_by_tags:\n",
    "            data_by_tags[tag] = []\n",
    "        data_by_tags[tag].append(d)\n",
    "    return data_by_tags.values()\n",
    "\n",
    "\n",
    "separated_data = separate_data_by_tags(combined_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e8f715a-561f-406a-8d3a-80665e16cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# Lists to store training and testing data\n",
    "training_data = []\n",
    "testing_data = []\n",
    "\n",
    "# Split each tag's data into training and testing sets\n",
    "combined_data = []\n",
    "for tag_data in separated_data:\n",
    "    combined_data.extend(tag_data)\n",
    "\n",
    "if len(combined_data) > 1:\n",
    "    train_data, test_data = train_test_split(combined_data, test_size=0.2, random_state=42)\n",
    "    training_data.extend(train_data)\n",
    "    testing_data.extend(test_data)\n",
    "else:\n",
    "    print(\"Insufficient data for splitting.\")\n",
    "\n",
    "\n",
    "random.shuffle(training_data)\n",
    "random.shuffle(testing_data)\n",
    "\n",
    "# Convert training and testing data back to np.array\n",
    "train_x = np.array([d[0] for d in training_data])\n",
    "train_y = np.array([d[1] for d in training_data])\n",
    "test_x = np.array([d[0] for d in testing_data])\n",
    "test_y = np.array([d[1] for d in testing_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e5af77-7f64-4382-9c45-2c79e4e5ba08",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_size)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "\n",
    "        self.fc3 = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        output = self.softmax(x)\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return self.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0df480-09f3-4eff-89a7-6a2d34942dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52fcbbd-2f87-4465-8546-26766104fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, targets):\n",
    "    predicted_labels = torch.argmax(predictions, dim=1)\n",
    "    true_labels = torch.argmax(targets, dim=1)\n",
    "    correct = (predicted_labels == true_labels).sum().item()\n",
    "    total = targets.size(0)\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4f0343-ccb5-4d64-8a9f-12580bde5577",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    total_accuracy = 0.0\n",
    "    num_batches = len(test_loader)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item() * inputs.size(0)\n",
    "            total_accuracy += accuracy(outputs, targets) * inputs.size(0)\n",
    "\n",
    "    average_loss = total_loss / len(test_loader.dataset)\n",
    "    average_accuracy = total_accuracy / len(test_loader.dataset)\n",
    "    return average_loss, average_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8918f160-0bdd-4740-b9cd-bd4ac19fbd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataLoader for training and testing data\n",
    "train_x = torch.tensor(train_x).float()\n",
    "train_y = torch.tensor(train_y).float()\n",
    "test_x = torch.tensor(test_x).float()\n",
    "test_y = torch.tensor(test_y).float()\n",
    "\n",
    "batch_size = 64\n",
    "train_dataset = CustomDataset(train_x, train_y)\n",
    "test_dataset = CustomDataset(test_x, test_y)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the model, loss function, and optimizer\n",
    "input_size = len(train_x[0])\n",
    "hidden_size = 8\n",
    "output_size = len(train_y[0])\n",
    "model = NeuralNetwork(input_size, hidden_size, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c2689f-1b8b-4777-8140-bacfc893c52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model and evaluate on the testing set\n",
    "num_epochs = 250\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    running_acc = 0.0\n",
    "    for inputs, targets in train_loader:\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        running_acc += accuracy(outputs, targets) * inputs.size(0)\n",
    "\n",
    "    # Calculate average training loss and accuracy for the epoch\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    epoch_acc = running_acc / len(train_loader.dataset)\n",
    "\n",
    "    # Print training loss and accuracy for each epoch\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.4f}\")\n",
    "\n",
    "    # Evaluate on the testing set\n",
    "    test_loss, test_accuracy = test_model(model, test_loader, criterion)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Testing Loss: {test_loss:.4f}, Testing Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440c730e-acdb-4827-ae91-b02fc671c978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the trained model\n",
    "torch.save(model.state_dict(), 'data.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b2967fa-b76f-410a-99f2-ca973d4de2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
